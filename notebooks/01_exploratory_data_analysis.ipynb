{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# src/data/preprocess.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def load_data(filepath):\n",
    "    \"\"\"Load data from CSV file.\"\"\"\n",
    "    logger.info(f\"Loading data from {filepath}\")\n",
    "    try:\n",
    "        data = pd.read_csv(filepath)\n",
    "        logger.info(f\"Data loaded successfully with shape {data.shape}\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading data: {e}\")\n",
    "        raise\n",
    "\n",
    "def check_missing_values(data):\n",
    "    \"\"\"Check for missing values in the dataframe.\"\"\"\n",
    "    missing = data.isnull().sum()\n",
    "    if missing.sum() > 0:\n",
    "        logger.warning(f\"Missing values found:\\n{missing[missing > 0]}\")\n",
    "    else:\n",
    "        logger.info(\"No missing values found\")\n",
    "    return missing\n",
    "\n",
    "def handle_outliers(data, columns, method='iqr', threshold=1.5):\n",
    "    \"\"\"Handle outliers in specified columns.\"\"\"\n",
    "    data_clean = data.copy()\n",
    "    \n",
    "    for col in columns:\n",
    "        if method == 'iqr':\n",
    "            Q1 = data_clean[col].quantile(0.25)\n",
    "            Q3 = data_clean[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - threshold * IQR\n",
    "            upper_bound = Q3 + threshold * IQR\n",
    "            \n",
    "            # Log outliers\n",
    "            outliers = data_clean[(data_clean[col] < lower_bound) | (data_clean[col] > upper_bound)]\n",
    "            if not outliers.empty:\n",
    "                logger.info(f\"Found {len(outliers)} outliers in column {col}\")\n",
    "            \n",
    "            # Cap outliers\n",
    "            data_clean[col] = np.where(\n",
    "                data_clean[col] < lower_bound,\n",
    "                lower_bound,\n",
    "                np.where(data_clean[col] > upper_bound, upper_bound, data_clean[col])\n",
    "            )\n",
    "    \n",
    "    return data_clean\n",
    "\n",
    "def preprocess_data(data, target_col='default', test_size=0.2, random_state=42):\n",
    "    \"\"\"Preprocess data for modeling.\"\"\"\n",
    "    logger.info(\"Starting data preprocessing\")\n",
    "    \n",
    "    # Check for missing values\n",
    "    check_missing_values(data)\n",
    "    \n",
    "    # Extract features and target\n",
    "    X = data.drop(columns=[target_col, 'customer_id'])  # Assuming customer_id is not needed\n",
    "    y = data[target_col]\n",
    "    \n",
    "    logger.info(f\"Features shape: {X.shape}, Target shape: {y.shape}\")\n",
    "    \n",
    "    # Handle outliers in numerical columns\n",
    "    numerical_cols = X.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "    X = handle_outliers(X, numerical_cols)\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, stratify=y\n",
    "    )\n",
    "    logger.info(f\"Train set: {X_train.shape}, Test set: {X_test.shape}\")\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    logger.info(\"Data preprocessing completed\")\n",
    "    \n",
    "    return X_train_scaled, X_test_scaled, y_train, y_test, scaler\n",
    "\n",
    "def save_processed_data(X_train, X_test, y_train, y_test, scaler, output_dir):\n",
    "    \"\"\"Save processed data to disk.\"\"\"\n",
    "    logger.info(f\"Saving processed data to {output_dir}\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Convert to DataFrames for saving\n",
    "    X_train_df = pd.DataFrame(X_train)\n",
    "    X_test_df = pd.DataFrame(X_test)\n",
    "    \n",
    "    # Save data\n",
    "    X_train_df.to_csv(f\"{output_dir}/X_train.csv\", index=False)\n",
    "    X_test_df.to_csv(f\"{output_dir}/X_test.csv\", index=False)\n",
    "    y_train.to_csv(f\"{output_dir}/y_train.csv\", index=False)\n",
    "    y_test.to_csv(f\"{output_dir}/y_test.csv\", index=False)\n",
    "    \n",
    "    # Save scaler\n",
    "    import joblib\n",
    "    joblib.dump(scaler, f\"{output_dir}/scaler.pkl\")\n",
    "    \n",
    "    logger.info(\"All processed data saved successfully\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    input_file = \"data/raw/Loan_Data.csv\"\n",
    "    output_dir = \"data/processed\"\n",
    "    \n",
    "    data = load_data(input_file)\n",
    "    X_train, X_test, y_train, y_test, scaler = preprocess_data(data)\n",
    "    save_processed_data(X_train, X_test, y_train, y_test, scaler, output_dir)\n",
    "\n",
    "Now, let's create a script to perform EDA:\n",
    "\n",
    "# src/data/eda.py\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def load_data(filepath):\n",
    "    \"\"\"Load data from CSV file.\"\"\"\n",
    "    logger.info(f\"Loading data from {filepath}\")\n",
    "    try:\n",
    "        data = pd.read_csv(filepath)\n",
    "        logger.info(f\"Data loaded successfully with shape {data.shape}\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading data: {e}\")\n",
    "        raise\n",
    "\n",
    "def explore_data(data, output_dir):\n",
    "    \"\"\"Perform exploratory data analysis and save visualizations.\"\"\"\n",
    "    logger.info(\"Starting exploratory data analysis\")\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Basic statistics\n",
    "    logger.info(\"Generating basic statistics\")\n",
    "    stats = data.describe().T\n",
    "    stats.to_csv(f\"{output_dir}/basic_statistics.csv\")\n",
    "    \n",
    "    # Check target distribution\n",
    "    logger.info(\"Analyzing target distribution\")\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    default_counts = data['default'].value_counts()\n",
    "    sns.barplot(x=default_counts.index, y=default_counts.values)\n",
    "    plt.title('Distribution of Default vs Non-Default')\n",
    "    plt.xlabel('Default Status (1=Default, 0=Non-Default)')\n",
    "    plt.ylabel('Count')\n",
    "    plt.savefig(f\"{output_dir}/target_distribution.png\")\n",
    "    \n",
    "    # Correlation matrix\n",
    "    logger.info(\"Generating correlation matrix\")\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    numeric_data = data.select_dtypes(include=['float64', 'int64'])\n",
    "    corr = numeric_data.corr()\n",
    "    sns.heatmap(corr, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "    plt.title('Correlation Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_dir}/correlation_matrix.png\")\n",
    "    \n",
    "    # Feature distributions by default status\n",
    "    logger.info(\"Analyzing feature distributions by default status\")\n",
    "    feature_cols = [col for col in data.columns if col not in ['customer_id', 'default']]\n",
    "    \n",
    "    for col in feature_cols:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.histplot(data=data, x=col, hue='default', kde=True, element='step')\n",
    "        plt.title(f'Distribution of {col} by Default Status')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{output_dir}/dist_{col}.png\")\n",
    "    \n",
    "    # Box plots for outlier detection\n",
    "    logger.info(\"Creating box plots for outlier detection\")\n",
    "    for col in feature_cols:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.boxplot(data=data, x='default', y=col)\n",
    "        plt.title(f'Box Plot of {col} by Default Status')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{output_dir}/boxplot_{col}.png\")\n",
    "    \n",
    "    # Generate summary report\n",
    "    logger.info(\"Generating data summary report\")\n",
    "    with open(f\"{output_dir}/eda_summary.txt\", 'w') as f:\n",
    "        f.write(f\"Dataset Shape: {data.shape}\\n\\n\")\n",
    "        f.write(f\"Data Types:\\n{data.dtypes}\\n\\n\")\n",
    "        f.write(f\"Missing Values:\\n{data.isnull().sum()}\\n\\n\")\n",
    "        f.write(f\"Target Distribution:\\n{data['default'].value_counts()}\\n\")\n",
    "        f.write(f\"Target Distribution (%):\\n{data['default'].value_counts(normalize=True) * 100}\\n\\n\")\n",
    "        \n",
    "    logger.info(\"EDA completed successfully\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = \"data/raw/Loan_Data.csv\"\n",
    "    output_dir = \"notebooks/eda_results\"\n",
    "    \n",
    "    data = load_data(input_file)\n",
    "    explore_data(data, output_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
